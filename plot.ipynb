{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f3b4d0-fa40-49ea-b2f5-80d79df292f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: lightly in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 2)) (1.5.3)\n",
      "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 3)) (2.2.4)\n",
      "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 4)) (0.5.6)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 5)) (0.13.2)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r req.txt (line 6)) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->-r req.txt (line 1)) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r req.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r req.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r req.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from lightly->-r req.txt (line 2)) (2022.12.7)\n",
      "Requirement already satisfied: hydra-core>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from lightly->-r req.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: lightly-utils~=0.0.0 in /usr/local/lib/python3.10/dist-packages (from lightly->-r req.txt (line 2)) (0.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from lightly->-r req.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from lightly->-r req.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.10/dist-packages (from lightly->-r req.txt (line 2)) (4.66.4)\n",
      "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from lightly->-r req.txt (line 2)) (1.26.13)\n",
      "Requirement already satisfied: pydantic<2,>=1.10.5 in /usr/local/lib/python3.10/dist-packages (from lightly->-r req.txt (line 2)) (1.10.15)\n",
      "Requirement already satisfied: aenum>=3.1.11 in /usr/local/lib/python3.10/dist-packages (from lightly->-r req.txt (line 2)) (3.1.15)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from lightly->-r req.txt (line 2)) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from lightly->-r req.txt (line 2)) (0.16.0+cu118)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning->-r req.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning->-r req.txt (line 3)) (2023.4.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning->-r req.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning->-r req.txt (line 3)) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning->-r req.txt (line 3)) (4.4.0)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning->-r req.txt (line 3)) (0.11.2)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn->-r req.txt (line 4)) (1.13.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn->-r req.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn->-r req.txt (line 4)) (0.59.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn->-r req.txt (line 4)) (0.5.12)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn->-r req.txt (line 5)) (3.8.4)\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r req.txt (line 6)) (3.0)\n",
      "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r req.txt (line 6)) (9.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r req.txt (line 6)) (2.34.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r req.txt (line 6)) (2024.5.10)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r req.txt (line 6)) (0.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning->-r req.txt (line 3)) (3.9.5)\n",
      "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.0.0->lightly->-r req.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.0.0->lightly->-r req.txt (line 2)) (4.9.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch_lightning->-r req.txt (line 3)) (68.2.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r req.txt (line 5)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r req.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r req.txt (line 5)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r req.txt (line 5)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn->-r req.txt (line 5)) (2.4.7)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn->-r req.txt (line 4)) (0.42.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn->-r req.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->lightly->-r req.txt (line 2)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->lightly->-r req.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn->-r req.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->lightly->-r req.txt (line 2)) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->lightly->-r req.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->lightly->-r req.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->lightly->-r req.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: pretty-errors==1.2.25 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning->-r req.txt (line 3)) (1.2.25)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from pretty-errors==1.2.25->torchmetrics>=0.7.0->pytorch_lightning->-r req.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->-r req.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->-r req.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->-r req.txt (line 3)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->-r req.txt (line 3)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->-r req.txt (line 3)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning->-r req.txt (line 3)) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->lightly->-r req.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->lightly->-r req.txt (line 2)) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r req.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492a87cb-5304-4d85-af3e-6dffd7dc55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing base python dependencies\n",
    "from argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from skimage.io import imread\n",
    "import pdb\n",
    "import pytorch_lightning as pl\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "from lightly.models.modules.heads import DINOProjectionHead\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54c51d7-a409-4913-b81a-b6a667ec9bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# This source code is licensed under the Apache License, Version 2.0\n",
    "# found in the LICENSE file in the root directory of this source tree.\n",
    "\n",
    "# References:\n",
    "#   https://github.com/facebookresearch/dino/blob/master/vision_transformer.py\n",
    "#   https://github.com/rwightman/pytorch-image-models/tree/master/timm/layers/patch_embed.py\n",
    "\n",
    "from typing import Callable, Optional, Tuple, Union\n",
    "\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def make_2tuple(x):\n",
    "    if isinstance(x, tuple):\n",
    "        assert len(x) == 2\n",
    "        return x\n",
    "\n",
    "    assert isinstance(x, int)\n",
    "    return (x, x)\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    2D image to patch embedding: (B,C,H,W) -> (B,N,D)\n",
    "\n",
    "    Args:\n",
    "        img_size: Image size.\n",
    "        patch_size: Patch token size.\n",
    "        in_chans: Number of input image channels.\n",
    "        embed_dim: Number of linear projection output channels.\n",
    "        norm_layer: Normalization layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: Union[int, Tuple[int, int]] = 224,\n",
    "        patch_size: Union[int, Tuple[int, int]] = 16,\n",
    "        in_chans: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "        norm_layer: Optional[Callable] = None,\n",
    "        flatten_embedding: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        image_HW = make_2tuple(img_size)\n",
    "        patch_HW = make_2tuple(patch_size)\n",
    "        patch_grid_size = (\n",
    "            image_HW[0] // patch_HW[0],\n",
    "            image_HW[1] // patch_HW[1],\n",
    "        )\n",
    "\n",
    "        self.img_size = image_HW\n",
    "        self.patch_size = patch_HW\n",
    "        self.patches_resolution = patch_grid_size\n",
    "        self.num_patches = patch_grid_size[0] * patch_grid_size[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.flatten_embedding = flatten_embedding\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_HW, stride=patch_HW)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        _, _, H, W = x.shape\n",
    "        patch_H, patch_W = self.patch_size\n",
    "\n",
    "        assert H % patch_H == 0, f\"Input image height {H} is not a multiple of patch height {patch_H}\"\n",
    "        assert W % patch_W == 0, f\"Input image width {W} is not a multiple of patch width: {patch_W}\"\n",
    "\n",
    "        x = self.proj(x)  # B C H W\n",
    "        H, W = x.size(2), x.size(3)\n",
    "        x = x.flatten(2).transpose(1, 2)  # B HW C\n",
    "        x = self.norm(x)\n",
    "        if not self.flatten_embedding:\n",
    "            x = x.reshape(-1, H, W, self.embed_dim)  # B H W C\n",
    "        return x\n",
    "\n",
    "    def flops(self) -> float:\n",
    "        Ho, Wo = self.patches_resolution\n",
    "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n",
    "        if self.norm is not None:\n",
    "            flops += Ho * Wo * self.embed_dim\n",
    "        return flops\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e3785ba-41d2-42c7-8718-9c42e9dd5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOViT(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 lr=1e-3,\n",
    "                 max_epoch_number=500,\n",
    "                 num_register_tokens=4,\n",
    "                 num_patches=1369, #1369   - [518/14 = 37 * 37 =1369]\n",
    "                 proj_dim=2048):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.max_epoch_number = max_epoch_number\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, 768))\n",
    "        assert num_register_tokens >= 0\n",
    "        dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg').cuda()\n",
    "        self.cls_token = dinov2_model.cls_token\n",
    "        self.register_tokens = dinov2_model.register_tokens\n",
    "        # Custom Patch Embed\n",
    "        self.patch_embed = PatchEmbed(img_size=518, patch_size=14)\n",
    "        # self.patch_embed = dinov2_model.patch_embed\n",
    "        self.student_backbone = nn.Sequential(*dinov2_model.blocks)\n",
    "        self.student_head = DINOProjectionHead(768, 2048, 256, proj_dim, batch_norm=False)\n",
    "        self.teacher_backbone = copy.deepcopy(self.student_backbone)\n",
    "        self.teacher_head = DINOProjectionHead(768, 2048, 256, proj_dim, batch_norm=False)\n",
    "        # Making the teacher model require no\n",
    "        for param in self.teacher_backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.teacher_head.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.dino_loss = DINOLossSingleViews(output_dim=proj_dim, warmup_teacher_temp_epochs=5)\n",
    "        \n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\"DINOViT\")\n",
    "        parser.add_argument(\"--lr\", type=float, default=1e-3, help=\"Learning rate for training.\")\n",
    "        parser.add_argument(\"--max_epoch_number\", type=int, default=500, help=\"Maximum number of epochs.\")\n",
    "        parser.add_argument(\"--proj_dim\", type=int, default=1024, help=\"The embedding dimension from the DINO head.\")\n",
    "        parser.add_argument(\"--num_register_tokens\", type=int, default=4, help=\"Number of register tokens.\")\n",
    "        parser.add_argument(\"--num_patches\", type=int, default=256, help=\"The number of patch tokens for the ViT.\")\n",
    "        return parent_parser\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.prepare_tokens_with_masks(X)\n",
    "        h = self.student_backbone(X)\n",
    "        # Extracting out the cls token\n",
    "        h = h[:, 0]\n",
    "        z = self.student_head(h)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, X):\n",
    "        X = self.prepare_tokens_with_masks(X)\n",
    "        h = self.teacher_backbone(X)\n",
    "        # Extracting out the cls token\n",
    "        h = h[:, 0]\n",
    "        z = self.teacher_head(h)\n",
    "        return z\n",
    "\n",
    "    def prepare_tokens_with_masks(self, x, masks=None):\n",
    "        \"\"\"\n",
    "        This function is adapted from DINOV2 from meta.\n",
    "        https://github.com/facebookresearch/dinov2/blob/main/dinov2/models/vision_transformer.py\n",
    "        \"\"\"\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed.repeat(B, 1, 1)\n",
    "        if masks is not None:\n",
    "            x = torch.where(masks.unsqueeze(-1), self.mask_token.to(x.dtype).unsqueeze(0), x)\n",
    "        \n",
    "        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "\n",
    "        if self.register_tokens is not None:\n",
    "            x = torch.cat(\n",
    "                (\n",
    "                    x[:, :1],\n",
    "                    self.register_tokens.expand(x.shape[0], -1, -1),\n",
    "                    x[:, 1:],\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "        return x\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_momentum(self, model: nn.Module, model_ema: nn.Module, m: float):\n",
    "        \"\"\"\n",
    "        Updates model_ema with Exponential Moving Average of model\n",
    "        \"\"\"\n",
    "        for model_ema, model in zip(model_ema.parameters(), model.parameters()):\n",
    "            model_ema.data = model_ema.data * m + model.data * (1.0 - m)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _calculate_nuc_norm(self, embeddings):\n",
    "        embeddings = embeddings.to(torch.float)\n",
    "        _, S, _ = torch.linalg.svd(embeddings)\n",
    "        nuc_norm = S.sum()\n",
    "        nuc_norm = -1 * nuc_norm\n",
    "        return nuc_norm\n",
    "    \n",
    "    def training_step(self, batch, _):\n",
    "        X, X_t = batch\n",
    "        momentum = cosine_schedule(self.current_epoch, 10, 0.996, 1)\n",
    "        # EMA update of backbone and head\n",
    "        self.update_momentum(self.student_backbone, self.teacher_backbone, m=momentum)\n",
    "        self.update_momentum(self.student_head, self.teacher_head, m=momentum)\n",
    "        # Forward Pass through teacher\n",
    "        teacher_out = self.forward_teacher(X_t)\n",
    "        # Detaching teacher output for stop gradient\n",
    "        teacher_out = teacher_out.detach()\n",
    "        # Forward Pass for student\n",
    "        student_out = self.forward(X)\n",
    "        # Calculating the loss\n",
    "        loss_dino = self.dino_loss(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        loss = loss_dino\n",
    "        # Calculating the negative nuclear norm to asses representational collapse\n",
    "        neg_nuclear_norm = self._calculate_nuc_norm(torch.vstack([student_out, teacher_out]))\n",
    "        loss_dict = {'train_loss': loss, 'train_nuc_norm': neg_nuclear_norm}\n",
    "        self.log_dict(loss_dict, on_step=True, on_epoch=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        X, X_t = batch\n",
    "        # Forward Pass through teacher\n",
    "        teacher_out = self.forward_teacher(X_t)\n",
    "        # Detaching teacher output for stop gradient\n",
    "        teacher_out = teacher_out.detach()\n",
    "        # Forward Pass for student\n",
    "        student_out = self.forward(X)\n",
    "        # Calculating the loss\n",
    "        loss_dino = self.dino_loss(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        loss = loss_dino\n",
    "        # Calculating the negative nuclear norm to asses representational collapse\n",
    "        neg_nuclear_norm = self._calculate_nuc_norm(torch.vstack([student_out, teacher_out]))\n",
    "        loss_dict = {'val_loss': loss, 'val_nuc_norm': neg_nuclear_norm}\n",
    "        self.log_dict(loss_dict, on_step=True, on_epoch=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        def warmup_fn(epoch):\n",
    "            warmup_epochs = 500\n",
    "            if epoch < warmup_epochs:\n",
    "                return (epoch / warmup_epochs)*self.lr\n",
    "            elif epoch == 0:\n",
    "                return (0.5 / warmup_epochs)*self.lr\n",
    "            else:\n",
    "                return self.lr\n",
    "        # Adding warmup scheduler\n",
    "        warmup_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_fn)\n",
    "        # After warmup, use a scheduler of your choice\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.max_epoch_number)\n",
    "        return [optimizer], [{\"scheduler\": warmup_scheduler, \"interval\": \"step\"}, {\"scheduler\": scheduler, \"interval\": \"epoch\"}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf709e3-1ccc-49a2-9c11-5e5d150c6993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSIPathSSLDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.img_files = pd.read_csv(csv_file)\n",
    "        self.length = len(self.img_files)\n",
    "        self.resize_transform = transforms.Resize((518, 518), antialias=True)\n",
    "        #518 x518\n",
    "\n",
    "    def _random_transform(self, image):\n",
    "        # Define a list of PyTorch transform functions\n",
    "        transform_functions = [\n",
    "            transforms.RandomHorizontalFlip(p=1),\n",
    "            transforms.RandomRotation([-90, 90]),\n",
    "            transforms.RandomVerticalFlip(p=1),\n",
    "            # Assuming RGBPerturbStainConcentrationTransform is a custom transform\n",
    "            # RGBPerturbStainConcentrationTransform(sigma1=0.7, sigma2=0.7)\n",
    "        ]\n",
    "        \n",
    "        # Apply the transform function to the image\n",
    "        n_transforms = random.choice([1, 2, 3])\n",
    "        for _ in range(n_transforms):\n",
    "            try:\n",
    "                transform_function = random.choice(transform_functions)\n",
    "                image = transform_function(image)\n",
    "            except:\n",
    "                pass\n",
    "        # Return the transformed images as a tensor\n",
    "        transformed_image = image\n",
    "        return transformed_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image_path = self.img_files.iloc[idx, 0]  # Assuming the first column contains image file paths\n",
    "        # image_path = image_path.split('/gladstone/finkbeiner/steve')\n",
    "        # image_path = '/Volumes/Finkbeiner-Steve' + image_path[1]\n",
    "        im = imread(image_path)\n",
    "     \n",
    "        im = torch.from_numpy(im)\n",
    "        im = im.permute(2, 0, 1) # bring channels first\n",
    "        \n",
    "        # Assuming 'root' and 'ii' are not defined here\n",
    "        # im = torch.Tensor(np.array(root['arr_0'][ii])).permute(2, 0, 1)\n",
    "\n",
    "        t_im = self._random_transform(im)\n",
    "        im = self.resize_transform(im)/255\n",
    "        t_im = self.resize_transform(t_im)/255\n",
    "        return im, t_im\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "\n",
    "\n",
    "class DINOLossSingleViews(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim: int = 65536,\n",
    "        warmup_teacher_temp: float = 0.04,\n",
    "        teacher_temp: float = 0.07,\n",
    "        warmup_teacher_temp_epochs: int = 10,\n",
    "        student_temp: float = 0.1,\n",
    "        center_momentum: float = 0.9,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.warmup_teacher_temp_epochs = warmup_teacher_temp_epochs\n",
    "        self.teacher_temp = teacher_temp\n",
    "        self.student_temp = student_temp\n",
    "        self.center_momentum = center_momentum\n",
    "        self.register_buffer(\"center\", torch.zeros(1, output_dim))\n",
    "        self.teacher_temp_schedule = torch.linspace(\n",
    "            start=warmup_teacher_temp,\n",
    "            end=teacher_temp,\n",
    "            steps=warmup_teacher_temp_epochs,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        teacher_out: torch.Tensor,\n",
    "        student_out: torch.Tensor,\n",
    "        epoch: int,\n",
    "        validation: bool = False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Cross-entropy between softmax outputs of the teacher and student\n",
    "        networks.\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "            teacher_out:\n",
    "                feature tensors from the teacher model. Each tensor is assumed \n",
    "                to contain features from one view of the batch and have length batch_size.\n",
    "            student_out:\n",
    "                feature tensors from the student model. Each tensor is assumed \n",
    "                to contain features from one view of the batch and have length batch_size.\n",
    "            epoch:\n",
    "                The current training epoch.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            The average cross-entropy loss.\n",
    "\n",
    "        \"\"\"\n",
    "        # get teacher temperature\n",
    "        if epoch < self.warmup_teacher_temp_epochs:\n",
    "            teacher_temp = self.teacher_temp_schedule[epoch]\n",
    "        else:\n",
    "            teacher_temp = self.teacher_temp\n",
    "\n",
    "        t_out = F.softmax((teacher_out - self.center) / teacher_temp, dim=-1)\n",
    "        s_out = F.log_softmax(student_out / self.student_temp, dim=-1)\n",
    "        t_out = t_out.unsqueeze(0)\n",
    "        s_out = s_out.unsqueeze(0)\n",
    "        \n",
    "        # calculate feature similarities where:\n",
    "        # b -> batch_size, t -> n_views_teacher, s -> n_views_student, d -> output_dim\n",
    "        loss = -torch.einsum(\"tbd,sbd->ts\", t_out, s_out).squeeze()\n",
    "        loss = loss/teacher_out.shape[0]\n",
    "        if not validation:\n",
    "            self.update_center(teacher_out)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_center(self, teacher_out: torch.Tensor) -> None:\n",
    "        \"\"\"Moving average update of the center used for the teacher output.\n",
    "\n",
    "        Args:\n",
    "            teacher_out:\n",
    "                Stacked output from the teacher model.\n",
    "\n",
    "        \"\"\"\n",
    "        batch_center = torch.mean(teacher_out, dim=0, keepdim=True)\n",
    "        if dist.is_available() and dist.is_initialized():\n",
    "            dist.all_reduce(batch_center)\n",
    "            batch_center = batch_center / dist.get_world_size()\n",
    "\n",
    "        # ema update\n",
    "        self.center = self.center * self.center_momentum + batch_center * (\n",
    "            1 - self.center_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "409f7ce9-3710-4e74-90bd-fa0cdb010bbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = DINOViT(proj_dim=1024)\n",
    "GPU_NUM = 0\n",
    "CHECKPOINT = '/workspace/Projects/logs/amyb-ssl/5wnhj6vp/checkpoints/last.ckpt'\n",
    "checkpoint = torch.load(CHECKPOINT)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "model.to(GPU_NUM)\n",
    "\n",
    "# Check if multiple GPUs are available\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b135c6a4-fc7b-4759-ad74-f7835da195b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = '/workspace/Projects/semi-supervised-learning/train.csv'\n",
    "val_img_path = '/workspace/Projects/semi-supervised-learning/val.csv'\n",
    "\n",
    "\n",
    "train_dataset = WSIPathSSLDataset(train_img_path)\n",
    "val_dataset = WSIPathSSLDataset(val_img_path)\n",
    "train_dl = DataLoader(train_dataset, shuffle=False, batch_size=16, num_workers=18, persistent_workers=True)\n",
    "val_dl = DataLoader(val_dataset, shuffle=False, batch_size=16, num_workers=18, persistent_workers=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72ed81d9-ebd7-4998-bd60-2d3553451162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3094/3094 [12:25<00:00,  4.15it/s]\n",
      "100%|██████████| 344/344 [01:25<00:00,  4.02it/s]\n"
     ]
    }
   ],
   "source": [
    "split = []\n",
    "embeddings = []\n",
    "\n",
    "for batch in tqdm(train_dl):\n",
    "    X, _ = batch\n",
    "    out = model(X.to(GPU_NUM))\n",
    "    out = out.detach().cpu().numpy()\n",
    "    embeddings.append(out)\n",
    "    split.append(['Train']*X.shape[0])\n",
    "\n",
    "for batch in tqdm(val_dl):\n",
    "    X, _ = batch\n",
    "    out = model(X.to(GPU_NUM))\n",
    "    out = out.detach().cpu().numpy()\n",
    "    embeddings.append(out)\n",
    "    split.append(['Validation']*X.shape[0])\n",
    "\n",
    "split = np.hstack(split)\n",
    "# embeddings = np.load(\"embeddings.npy\")\n",
    "embeddings = np.vstack(embeddings)\n",
    "embeddings_norm = F.normalize(torch.Tensor(embeddings)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78770ff2-09db-4ced-bfc1-a37d10acf9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"embeddings-518.npy\", embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac3bb42-9ef9-4d28-9993-3e3d4ad1d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "# print(umap.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a47ec1c-8bb6-4653-bd0f-6d9c0278c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "\n",
    "\n",
    "u_obj = umap.UMAP(metric='cosine')\n",
    "u = u_obj.fit_transform(embeddings_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4037b85-532a-4706-908a-257f8f9df22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54996"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbbb610-4f7d-4b42-9454-cccd35252847",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rand_inds = np.random.choice(np.arange(len(u))[::40], 1000, replace=False)\n",
    "\n",
    "coordinates = u[rand_inds]\n",
    "\n",
    "\n",
    "images = [train_dataset[i][0].permute(1, 2, 0).numpy() if i < len(train_dataset) else val_dataset[i-len(train_dataset)][0].permute(1, 2, 0).numpy() for i in rand_inds]\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.scatterplot(x=u[: : 40, 0], y=u[::40, 1], s=1, alpha=0.05)\n",
    "ax = plt.gca()\n",
    "for coord, img in zip(coordinates, images):\n",
    "    imgbox = plt.matplotlib.offsetbox.OffsetImage(img, zoom=0.08, resample=True, clip_path=None)\n",
    "    ab = plt.matplotlib.offsetbox.AnnotationBbox(imgbox, coord, frameon=False, pad=0, xycoords='data', boxcoords=\"data\")\n",
    "    ax.add_artist(ab)\n",
    "plt.savefig('scatter-518.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e47ff1-d354-414e-99a3-d8485a4ff2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee759e0-de40-4fa7-8021-3aaa219d41e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
